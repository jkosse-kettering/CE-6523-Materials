{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94902774",
   "metadata": {},
   "source": [
    "# CE - 652\n",
    "## Artificial Intelligence for Autonomous Driving\n",
    "## Application Assignments \n",
    "##### Week: 8\n",
    "##### Instructor: Dr. Juan D. Gomez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ceea8d",
   "metadata": {},
   "source": [
    "# **Request: Enhancing This Jupyter Notebook with Key Transformer Concepts**\n",
    "\n",
    "## **Objective**\n",
    "Please enrich this Jupyter Notebook with a structured, interactive, and visually engaging **desk of contents** covering the following key topics:\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Transformer Architecture Overview**\n",
    "- Provide a concise yet comprehensive explanation of the Transformer model.\n",
    "- Utilize well-structured diagrams and animations to illustrate its key components, including:\n",
    "  - **Encoder-Decoder Structure**\n",
    "  - **Self-Attention Mechanisms**\n",
    "  - **Feedforward Layers**\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Positional Encodings**\n",
    "- Provide your own explanation of **positional encodings** \n",
    "- Include **visual representations** to facilitate intuitive understanding.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Attention Mechanisms**\n",
    "- Break down the following mechanisms step by step:\n",
    "  - **Scaled Dot-Product Attention**\n",
    "  - **Self-Attention**\n",
    "  - **Multi-Head Attention**\n",
    "- Incorporate **visualizations and real-world analogies** to demonstrate their role in context learning.\n",
    "\n",
    "---\n",
    "\n",
    "## **Guidelines**\n",
    "‚úÖ **Minimize extensive text** ‚Äî prioritize **figures, animations, and practical examples** to enhance comprehension.  \n",
    "‚úÖ **Optionally you can use interactive elements** where applicable (e.g., sliders, heatmaps, or attention visualizations) to allow dynamic exploration of concepts.  \n",
    "‚úÖ Ensure **clarity and an intuitive flow** that aligns with how Transformers are taught in modern deep learning curricula.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cb8c3f",
   "metadata": {},
   "source": [
    "# **Coding assignment:**\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2b33aa",
   "metadata": {},
   "source": [
    "Please create a Python function (using the template below) that simulates the Attention mechanism described in the paper \"Attention Is All You Need\". This function randomly generates inputs (queries, keys, and values), hardcodes weight matrices (also randomly generated), and computes the output of the Attention mechanism step by step.\n",
    "\n",
    "It follows the formula:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax} \\left( \\frac{Q K^T}{\\sqrt{d_k}} \\right) V\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "ùëÑ (queries), \n",
    "ùêæ (keys), and \n",
    "ùëâ (values) are randomly generated input matrices.\n",
    "\n",
    "The weight matrices for the projections of \n",
    "ùëÑ\n",
    ",\n",
    "ùêæ\n",
    ", and \n",
    "ùëâ are also randomly generated.\n",
    "\n",
    "The function then computes scaled dot-product attention and outputs the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a14d40e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def simulate_attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    Simulates the Attention mechanism from \"Attention Is All You Need\"\n",
    "    with hardcoded (randomly initialized) parameters.\n",
    "\n",
    "    Parameters:\n",
    "    Q (numpy.ndarray): Query matrix (seq_length, d_model)\n",
    "    K (numpy.ndarray): Key matrix (seq_length, d_model)\n",
    "    V (numpy.ndarray): Value matrix (seq_length, d_model)\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Attention output matrix\n",
    "    \"\"\"\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    \n",
    "    # Validate input dimensions \"Q, K, and V must have the same shape\"\n",
    "    #your code here \n",
    "    \n",
    "    # Extract sequence length and embedding dimension\n",
    "    # Your code here\n",
    "\n",
    "    # Hardcoded (randomly initialized) weight matrices\n",
    "    # your code here\n",
    "\n",
    "    # Apply learned transformations\n",
    "    # your code here\n",
    "\n",
    "    # Compute scaled dot-product attention\n",
    "    # Use Scaling factor\n",
    "    # your code here\n",
    "\n",
    "    # Apply softmax\n",
    "    # your code here\n",
    "\n",
    "    # Compute the final attention output\n",
    "    #your code here\n",
    "\n",
    "    return attention_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7df30416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated Attention Output:\n",
      "[[0.39262948 1.35442195 1.22699499 1.53990615]\n",
      " [0.40230947 1.38003129 1.25121885 1.56692975]\n",
      " [0.40299057 1.37916101 1.25199227 1.56584286]\n",
      " [0.39617445 1.36497352 1.23375705 1.55021601]\n",
      " [0.40423846 1.38312263 1.25186312 1.56910907]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage with random inputs\n",
    "seq_length = 5\n",
    "d_model = 4\n",
    "Q = np.random.rand(seq_length, d_model)\n",
    "K = np.random.rand(seq_length, d_model)\n",
    "V = np.random.rand(seq_length, d_model)\n",
    "\n",
    "output = simulate_attention(Q, K, V)\n",
    "print(\"Simulated Attention Output:\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2163e6",
   "metadata": {},
   "source": [
    "Now, can you use the same \"simulate_attention\" function to perform self-attention?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3205a48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated Self-Attention Output:\n",
      "[[0.49855453 1.53473981 1.30312152 1.80045753]\n",
      " [0.52012488 1.59878081 1.37676646 1.87154636]\n",
      " [0.41162108 1.24101302 1.02000712 1.46154345]\n",
      " [0.47253647 1.45413035 1.21602026 1.7099343 ]\n",
      " [0.45663862 1.39503364 1.16383683 1.64030628]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage for self-attention with random inputs\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594e19ce",
   "metadata": {},
   "source": [
    "##### Please explain what the inputs and outputs (here random values) would mean in a real usage of attention and use an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d39d8e1",
   "metadata": {},
   "source": [
    "Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec18466",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
