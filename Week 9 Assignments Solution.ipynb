{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4260d0f",
   "metadata": {},
   "source": [
    "# CE - 652\n",
    "## Artificial Intelligence for Autonomous Driving\n",
    "## Application Assignments \n",
    "##### Week: 9\n",
    "##### Instructor: Dr. Juan D. Gomez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7507ab24",
   "metadata": {},
   "source": [
    "####  Excercise1:\n",
    "\n",
    "Please create a simple code in python to have a simple chat with chatGPT from within this Jupyter notebook using the OpenAI library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e7d8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "def chat_with_gpt(prompt, model=\"gpt-4\", api_key=\"your-api-key-here\"):\n",
    "    \"\"\"Function to interact with OpenAI's ChatGPT within Jupyter Notebook.\"\"\"\n",
    "    openai.api_key = api_key\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    user_input = input(\"You: \")\n",
    "    response = chat_with_gpt(user_input)\n",
    "    print(\"ChatGPT:\", response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db413d90",
   "metadata": {},
   "source": [
    "In the previous example, we connected to OpenAI's API since the LLM runs in the cloud. However, alternative LLMs, such as Llama, provide similar cloud-based inference while also offering downloadable models that can be deployed locally.\n",
    "\n",
    "These two tutorial demonstrate how to use Llama in the same manner as ChatGPT above, but with the advantage of a free API key. Esperiment with either of them:\n",
    "\n",
    "TUTORIAL 1: [How to Use Llama API](https://www.llama.com/products/llama-api/) \n",
    "TUTORIAL 2: [How to use Llama through Huggingface](https://huggingface.co/meta-llama)\n",
    "\n",
    "\n",
    "Additionally, this tutorial covers running Llama locally on your machine:\n",
    "\n",
    "TUTORIAL 2: [Setting Llama Locally](https://www.llama.com/)\n",
    "\n",
    "You are encouraged to experiment with either approach or both and report your experience with Llama here (i.e., the same chat you previously had with chatGPT) . However, be aware that the local deployment requires significant hardware resources, which most of you may not have. Nevertheless, feel free to attempt the local setup if you have the necessary computational capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846037f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from setuptools import find_packages, setup\n",
    "\n",
    "\n",
    "def get_requirements(path: str):\n",
    "    return [l.strip() for l in open(path)]\n",
    "\n",
    "\n",
    "setup(\n",
    "    name=\"llama\",\n",
    "    version=\"0.0.1\",\n",
    "    packages=find_packages(),\n",
    "    install_requires=get_requirements(\"requirements.txt\"),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
